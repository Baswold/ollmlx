# MLX Backend for ollmlx
# Provides MLX-based model inference as a subprocess service
# Maintains API compatibility with Ollama's CompletionRequest/Response format
